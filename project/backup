import cv2
import tkinter as tk
from PIL import Image, ImageTk
import numpy as np
from ultralytics import YOLO
import mysql.connector
import face_recognition

app = tk.Tk()
app.title("Face Check")

model = YOLO('yolov8n.pt')
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: Cannot open camera.")
    exit()

# Initialize counting system
count = 0
count_label = tk.Label(app, text=f"Count: {count}", font=("Arial", 20))
count_label.pack()

# Line position
line_x = 320  # Vertical line in the middle of a 640x480 video frame

# Buffer zone to detect crossing
buffer_zone = 20

# Track object states
crossed = set()  # Set to keep track of objects that have crossed the line

def load_face_data():
    conn = mysql.connector.connect(
        host="localhost",
        user="root",
        password="",
        database="member"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT username, picture FROM users")
    face_data_db = cursor.fetchall()  # [(username, face_encoding), ...]
    
    known_face_encodings = []
    known_face_names = []
    
    for username, face_data in face_data_db:
        expected_size = 128 * 8  # 128 float64 values, each 8 bytes
        
        if len(face_data) == expected_size:
            face_encoding = np.frombuffer(face_data, dtype=np.float64)
            known_face_encodings.append(face_encoding)
            known_face_names.append(username)
        else:
            print(f"Error: Invalid face encoding size for {username}. Expected size {expected_size}, but got {len(face_data)}.")
    
    cursor.close()
    conn.close()
    
    return known_face_encodings, known_face_names

known_face_encodings, known_face_names = load_face_data()

def detect_person():
    global count, crossed
    check, frame = cap.read()
    if check:
        # Detect objects using YOLO
        results = model(frame)
        
        # Print results to debug
        print("YOLO Results:", results)
        print("Results type:", type(results))
        print("Boxes:", results[0].boxes.xyxy)  # Print the bounding boxes
        
        # Extract bounding boxes and class IDs
        boxes = results[0].boxes
        current_boxes = set()  # Set to store bounding boxes of the current frame
        detected_names = []  # List to store names of detected faces
        
        for box in boxes:
            x_min, y_min, x_max, y_max = map(int, box.xyxy[0].tolist())
            class_id = int(box.cls[0])
            if class_id == 0:  # Assuming class_id 0 corresponds to 'person'
                # Draw bounding box
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                
                # Determine if person crosses the line
                center_x = (x_min + x_max) // 2
                if center_x < line_x - buffer_zone:
                    # Person has crossed to the left
                    if (x_min, y_min, x_max, y_max) not in crossed:
                        if count > 0:
                            count -= 1
                            count_label.config(text=f"Count: {count}")
                        crossed.add((x_min, y_min, x_max, y_max))
                elif center_x > line_x + buffer_zone:
                    # Person has crossed to the right
                    if (x_min, y_min, x_max, y_max) not in crossed:
                        count += 1
                        count_label.config(text=f"Count: {count}")
                        crossed.add((x_min, y_min, x_max, y_max))
                
                # Extract face region and recognize face
                face_image = frame[y_min:y_max, x_min:x_max]
                if face_image.size > 0:  # Check if face_image is not empty
                    rgb_face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
                    face_encodings = face_recognition.face_encodings(rgb_face_image)
                    if face_encodings:
                        face_encoding = face_encodings[0]
                        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
                        distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                        best_match_index = np.argmin(distances)
                        if matches[best_match_index]:
                            detected_name = known_face_names[best_match_index]
                            detected_names.append(detected_name)
                            cv2.putText(frame, detected_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
        
        # Update crossed objects
        crossed = current_boxes
        
        # Draw vertical line in the middle
        cv2.line(frame, (line_x, 0), (line_x, frame.shape[0]), (255, 0, 0), 2)
        
        # Convert frame to PIL Image and display in Tkinter
        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        imgtk = ImageTk.PhotoImage(image=img)
        label_video.imgtk = imgtk
        label_video.config(image=imgtk)
    
    app.after(10, detect_person)

# Display video feed
label_video = tk.Label(app)
label_video.pack()

# Start detection
detect_person()

# GUI loop
app.mainloop()

# Release camera when program closes
cap.release()
